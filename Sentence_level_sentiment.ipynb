{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from nltk import word_tokenize\n",
    "import numpy as np\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "from IPython.core.debugger import set_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_data():\n",
    "    \"\"\"\n",
    "    Return a list of strings (where words have spaces correctly between them) for each sentiment\n",
    "    \"\"\"\n",
    "    t = pd.read_table('./Datasets/Rotten_Tomatoes/train.tsv')\n",
    "#     negative = t[t['Sentiment'] == 0].append(t[t['Sentiment'] == 1])\n",
    "    negative = t[t['Sentiment'] == 0]\n",
    "    negative.sort_values('PhraseId', inplace=True)\n",
    "    negative.drop_duplicates('SentenceId', keep='first', inplace=True)\n",
    "    \n",
    "#     neutral = t[t['Sentiment'] == 2]\n",
    "    neutral = t[t['Sentiment'] == 1].append(t[t['Sentiment'] == 2]).append(t[t['Sentiment'] == 3])\n",
    "    neutral.sort_values('PhraseId', inplace=True)\n",
    "    neutral.drop_duplicates('SentenceId', keep='first', inplace=True)\n",
    "    \n",
    "#     positive = t[t['Sentiment'] == 3].append(t[t['Sentiment'] == 4])\n",
    "    positive = t[t['Sentiment'] == 4]\n",
    "    positive.sort_values('PhraseId', inplace=True)\n",
    "    positive.drop_duplicates('SentenceId', keep='first', inplace=True)\n",
    "    \n",
    "    return negative['Phrase'].tolist(), neutral['Phrase'].tolist(), positive['Phrase'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/akkash/.local/lib/python3.5/site-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/home/akkash/.local/lib/python3.5/site-packages/ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if __name__ == '__main__':\n",
      "/home/akkash/.local/lib/python3.5/site-packages/ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/akkash/.local/lib/python3.5/site-packages/ipykernel_launcher.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2609, 8470, 3123)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negative_reviews, neutral_reviews, positive_reviews = get_train_data()\n",
    "min_size = min([len(negative_reviews), len(neutral_reviews), len(positive_reviews)])\n",
    "negative_reviews_train = negative_reviews #[:min_size]\n",
    "neutral_reviews_train = neutral_reviews #[:min_size]\n",
    "positive_reviews_train = positive_reviews #[:min_size]\n",
    "# Already tokenised, so use .split() method to get tokens\n",
    "len(negative_reviews_train), len(neutral_reviews_train), len(positive_reviews_train)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenise_document(review, add_not_tag=False, unigram=False, bigram=False):\n",
    "    \"\"\"\n",
    "    Given a review (string) we return a list of tokens\n",
    "    \"\"\"\n",
    "    \n",
    "    review_split = word_tokenize(review)\n",
    "    tokens = []\n",
    "    \n",
    "    if add_not_tag:\n",
    "        negation_word = {'not': True, 'never':True, \"isn't\": True, \"doesn't\": True, \"wasn't\":True, \"couldn't\": True, \"wouldn't\": True, \n",
    "                         \"didn't\": True}\n",
    "        punctuation = {'?': True, '!': True, '.': True, ',': True, ':': True, ';':True}\n",
    "\n",
    "        convert_word = False\n",
    "        for word in review_split:\n",
    "            if word in punctuation:\n",
    "                convert_word = False\n",
    "                tokens.append(word)\n",
    "                continue\n",
    "\n",
    "            if convert_word:\n",
    "                tokens.append('NOT_'+word)\n",
    "                continue\n",
    "\n",
    "            if word in negation_word:\n",
    "                convert_word = True\n",
    "                tokens.append(word)\n",
    "                continue\n",
    "            tokens.append(word)\n",
    "                \n",
    "                \n",
    "    if unigram:\n",
    "        tokens.extend(review_split)\n",
    "        \n",
    "    if bigram:\n",
    "        # Since one of the above two conditions will be fulfilled, 'tokens' will always have entries\n",
    "        for index in range(len(tokens) - 1):\n",
    "            word_1 = tokens[index]\n",
    "            word_2 = tokens[index + 1]\n",
    "            tokens[index] = word_1 + ' ' + word_2\n",
    "        if len(tokens) > 0:\n",
    "            tokens.pop()  # Take the last unigram word at end of tokens list\n",
    "        \n",
    "    return tokens\n",
    "\n",
    "\n",
    "def get_vocabulary(add_not_tag=False, unigram=False, bigram=False, length=16162):\n",
    "    reviews = []\n",
    "    to_search = './Datasets/neg/'\n",
    "    for f in os.listdir(to_search):\n",
    "        path = to_search+f\n",
    "        with open(path, 'r', encoding='latin-1') as fin:\n",
    "            reviews.append(fin.read().strip())\n",
    "\n",
    "    to_search = './Datasets/pos/'\n",
    "    for f in os.listdir(to_search):\n",
    "        path = to_search+f\n",
    "        with open(path, 'r', encoding='latin-1') as fin:\n",
    "            reviews.append(fin.read().strip())\n",
    "    \n",
    "    data = negative_reviews + neutral_reviews + positive_reviews + reviews  # We have a list of textual reviews (strings)\n",
    "    \n",
    "    freq = {}  # Since we are using dict, vocabulary will be unique\n",
    "    for review in data:\n",
    "        review_tokens = tokenise_document(review, unigram=unigram, add_not_tag=add_not_tag, bigram=bigram)\n",
    "\n",
    "        for token in review_tokens:\n",
    "            if token in freq:\n",
    "                freq[token] += 1\n",
    "            else:\n",
    "                freq[token] = 1\n",
    "                \n",
    "    sorted_freq = sorted([(count, token) for token, count in freq.items()], reverse=True)\n",
    "    vocabulary = set([token for _, token in sorted_freq[:length]])  # We use set to exploit O(1) lookup time\n",
    "    \n",
    "    return vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayes:\n",
    "    \n",
    "    def __init__(self, vocabulary, tokenisation):\n",
    "        self.vocabulary = vocabulary\n",
    "        self.tokenisation = tokenisation\n",
    "\n",
    "        \n",
    "    def get_document_vector(self, document_tokens):\n",
    "        freq = {v:0 for v in self.vocabulary}\n",
    "        for word in document_tokens:\n",
    "            if word in self.vocabulary:\n",
    "                freq[word] = 1\n",
    "        return freq\n",
    "    \n",
    "    \n",
    "    def get_class_probabilities(self, class_data):\n",
    "        \"\"\"\n",
    "        class_data: A list containing all the tokens in a training data document. P(f_i|c) from training documents\n",
    "        \"\"\"\n",
    "        # We start with the add-one smoothing\n",
    "        log_prob_dict = {v:1 for v in self.vocabulary}  # We encode the add one smoothing count at start\n",
    "        \n",
    "        total_class_tokens = 0  # Tokens contained in vocabulary\n",
    "        for word in class_data:\n",
    "            if word in self.vocabulary:\n",
    "                log_prob_dict[word] += 1\n",
    "                total_class_tokens += 1\n",
    "                \n",
    "        # We divide by the denominator and log the values\n",
    "        for key in log_prob_dict.keys():\n",
    "            log_prob_dict[key] = np.log(log_prob_dict[key] / (total_class_tokens + len(self.vocabulary)))\n",
    "\n",
    "        return log_prob_dict\n",
    "\n",
    "    \n",
    "    def train_probabilities(self, negative_reviews, neutral_reviews, positive_reviews):\n",
    "        neg_train_tokens = []\n",
    "        for neg_review in negative_reviews:\n",
    "            neg_review = self.tokenisation(neg_review)\n",
    "            neg_train_tokens.extend(neg_review)\n",
    "        negative_log_probs = self.get_class_probabilities(neg_train_tokens)\n",
    "        \n",
    "        print(\"Trained neg\")\n",
    "        \n",
    "        pos_train_tokens = []\n",
    "        for pos_review in positive_reviews:\n",
    "            pos_review = self.tokenisation(pos_review)\n",
    "            pos_train_tokens.extend(pos_review)\n",
    "        positive_log_probs = self.get_class_probabilities(pos_train_tokens)\n",
    "        \n",
    "        print(\"Trained pos\")\n",
    "        \n",
    "        neutral_train_tokens = []\n",
    "        for neutral_review in neutral_reviews:\n",
    "            neutral_review = self.tokenisation(neutral_review)\n",
    "            neutral_train_tokens.extend(neutral_review)\n",
    "        neutral_log_probs = self.get_class_probabilities(neutral_train_tokens)\n",
    "        \n",
    "        print(\"Trained neutral\")\n",
    "    \n",
    "        return negative_log_probs, neutral_log_probs, positive_log_probs\n",
    "    \n",
    "    \n",
    "    def get_test_data(self):\n",
    "        reviews = []\n",
    "        labels = []\n",
    "        to_search = './Datasets/neg/'\n",
    "        for f in os.listdir(to_search):\n",
    "            path = to_search+f\n",
    "            with open(path, 'r', encoding='latin-1') as fin:\n",
    "                reviews.append(fin.read().strip())\n",
    "                \n",
    "        labels.extend([-1] * 700)\n",
    "\n",
    "        to_search = './Datasets/pos/'\n",
    "        for f in os.listdir(to_search):\n",
    "            path = to_search+f\n",
    "            with open(path, 'r', encoding='latin-1') as fin:\n",
    "                reviews.append(fin.read().strip())\n",
    "                \n",
    "        labels.extend([1] * 700)\n",
    "        \n",
    "                \n",
    "        reviews_sentences = list(map(lambda x: x.split('.'), reviews))\n",
    "        return reviews_sentences, labels\n",
    "    \n",
    "    \n",
    "    def test_single_document(self, negative_log_probs, neutral_log_probs, positive_log_probs, review):\n",
    "        review_sentences = test_review.split('.')\n",
    "        neg_sentences = []\n",
    "        neu_sentences = []\n",
    "        pos_sentences = []\n",
    "        \n",
    "        for sentence in review_sentences:\n",
    "            sentence_tokens = self.tokenisation(sentence)\n",
    "            sentence_vector = self.get_document_vector(sentence_tokens)\n",
    "            \n",
    "            neg_sum = 0   # For negative class\n",
    "            neu_sum = 0   # For neutral class\n",
    "            pos_sum = 0   # For positive class\n",
    "\n",
    "            for word, freq in sentence_vector.items():\n",
    "                # When we use counts in feature vector use below two    \n",
    "                if freq == 0:\n",
    "                    continue\n",
    "\n",
    "                neg_sum += negative_log_probs[word]\n",
    "                pos_sum += positive_log_probs[word]\n",
    "                neu_sum += neutral_log_probs[word]\n",
    "\n",
    "            if neg_sum > neu_sum and neg_sum > pos_sum:\n",
    "                neg_sentences.append(sentence)\n",
    "            elif neu_sum > neg_sum and neu_sum > pos_sum:\n",
    "                neu_sentences.append(sentence)\n",
    "            elif pos_sum > neg_sum and pos_sum > neu_sum:\n",
    "                pos_sentences.append(sentence)\n",
    "                \n",
    "        return neg_sentences, neu_sentences, pos_sentences\n",
    "    \n",
    "    \n",
    "    def test_documents(self, negative_log_probs, neutral_log_probs, positive_log_probs, add_full_stop=False):\n",
    "        correct = 0\n",
    "        \n",
    "        test_reviews, test_labels = self.get_test_data()\n",
    "        # test_review is a list of lists. Each inner list contains a list of sentences (string)\n",
    "        \n",
    "        counter = 0\n",
    "        \n",
    "        for review, label in zip(test_reviews, test_labels):\n",
    "            counter += 1\n",
    "            if counter % 100 == 0:\n",
    "                print(counter)\n",
    "                \n",
    "            neg_count = 0\n",
    "            pos_count = 0\n",
    "            \n",
    "            sentence_labels = []\n",
    "            for sentence in review:\n",
    "                if add_full_stop:\n",
    "                    sentence += '.'\n",
    "                sentence_tokens = self.tokenisation(sentence)\n",
    "                sentence_vector = self.get_document_vector(sentence_tokens)\n",
    "                \n",
    "                neg_sum = 0   # For negative class\n",
    "                neu_sum = 0   # For neutral class\n",
    "                pos_sum = 0   # For positive class\n",
    "\n",
    "                for word, freq in sentence_vector.items():\n",
    "                    # When we use counts in feature vector use below two    \n",
    "                    if freq == 0:\n",
    "                        continue\n",
    "\n",
    "                    neg_sum += negative_log_probs[word]\n",
    "                    pos_sum += positive_log_probs[word]\n",
    "                    neu_sum += neutral_log_probs[word]\n",
    "\n",
    "#                 if neg_sum > neu_sum and neg_sum > pos_sum:\n",
    "#                     sentence_labels.append(0)\n",
    "#                 elif neu_sum > neg_sum and neu_sum > pos_sum:\n",
    "#                     sentence_labels.append(1)\n",
    "#                 elif pos_sum > neg_sum and pos_sum > neu_sum:\n",
    "#                     sentence_labels.append(2)\n",
    "\n",
    "                if neg_sum > pos_sum and neg_sum > neu_sum:\n",
    "                    neg_count += 1\n",
    "                elif pos_sum > neg_sum and pos_sum > neu_sum:\n",
    "                    pos_count += 1\n",
    "                \n",
    "#             review_label = np.argmax(np.bincount(np.array(sentence_labels))) - 1  # -1 since we added 1 to labels for np.bincount\n",
    "\n",
    "            review_label = 3  # Some non-sensical value\n",
    "            if neg_count > pos_count:\n",
    "                review_label = -1\n",
    "            elif pos_count > neg_count:\n",
    "                review_label = 1\n",
    "            \n",
    "            if review_label == label:\n",
    "                correct += 1\n",
    "#             else:\n",
    "#                 print(counter)\n",
    "#                 print(\"Correct label:\", label, \"But we gave label:\", review_label)\n",
    "#                 print(neg_count, pos_count)\n",
    "                \n",
    "        return correct / len(test_reviews)\n",
    "    \n",
    "    \n",
    "    def get_statistics(self):\n",
    "        negative_log_probs, neutral_log_probs, positive_log_probs = self.train_probabilities(negative_reviews_train, neutral_reviews_train, positive_reviews_train)\n",
    "        print(\"TRAINED\")\n",
    "        accuracy = self.test_documents(negative_log_probs, neutral_log_probs, positive_log_probs, add_full_stop=False)\n",
    "        return accuracy\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32324\n"
     ]
    }
   ],
   "source": [
    "vocabulary_unigram = get_vocabulary(add_not_tag=True)\n",
    "vocabulary_bigram = get_vocabulary(unigram=True, bigram=True)\n",
    "vocabulary = vocabulary_unigram | vocabulary_bigram\n",
    "def tokenisation(x):\n",
    "    unigrams = tokenise_document(x, add_not_tag=True)\n",
    "    bigrams = tokenise_document(x, unigram=True, bigram=True)\n",
    "    unigrams.extend(bigrams)\n",
    "    return unigrams\n",
    "\n",
    "# vocabulary = get_vocabulary(unigram=True, bigram=True)\n",
    "# tokenisation = lambda x: tokenise_document(x, unigram=True, bigram=True)\n",
    "\n",
    "print(len(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained neg\n",
      "Trained pos\n",
      "Trained neutral\n"
     ]
    }
   ],
   "source": [
    "nb = NaiveBayes(vocabulary, tokenisation)\n",
    "negative_log_probs, neutral_log_probs, positive_log_probs = nb.train_probabilities(negative_reviews_train, neutral_reviews_train, positive_reviews_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6207142857142857"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb = NaiveBayes(vocabulary, tokenisation)\n",
    "nb.test_documents(negative_log_probs, neutral_log_probs, positive_log_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_review = \"I have now seen Mr. Tommy Wiseau's cinematic tour-de-force, 'The Room' three times. With each viewing, The Room becomes more complexly entangled in and inseparable from my own life. I no longer know where The Room ends and I begin. It is, without question, the worst film ever made. But this comment is in no way meant to be discouraging. Because while The Room is the worst movie ever made it is also the greatest way to spend a blisteringly fast 100 minutes in the dark. Simply put, The Room will change your life. It\\'s not just the dreadful acting or the sub-normal screenplay or the bewildering direction or the musical score so soaked in melodrama that you will throw up on yourself or the lunatic-making cinematography; no, there is something so magically wrong with this movie that it can only be the product of divine intervention. If you took the greatest filmmakers in history and gave them all the task of purposefully creating a film as spectacularly horrible as this not one of them, with all their knowledge and skill, could make anything that could even be considered as a contender. Not one line or scene would rival any moment in The Room. The centerpiece of this filmic holocaust is Mr. Tommy Wiseau himself. Without him, it would still be the worst movie ever made, but with him it is the greatest worst movie ever made. Tommy has been described as a Cajun, a Croatian cyborg, possibly from Belgium, clearly a product of Denmark, or maybe even not from this world or dimension. All of these things are true at any one moment. He is a tantalizing mystery stuffed inside an enigma wrapped in bacon and smothered in cheese. You will fall in love with this man even as you are repelled by him from the first moment he steps onto screen with his long Louis the Fourteenth style black locks and thick triangular shoulders packed into an oddly fitting suit, and his metallic steroid destroyed skin. Tommy looks out of place, out of time and out of this world. There has never been anything else like him. Nor will there ever be. The Room begins with Johnny (Tommy Wiseau) and his incomprehensibly evil fiancée Lisa (played by a woman with incongruously colored eyebrows and a propensity for removing her shirt) engaging in some light frottage, joined by, Denny, (played with a deft sense of the absurd by Phillip Haldiman), their sexually confused teenage neighbor who is clearly suffering from a form of aged decrepitude. When Denny, who looks like the human version of Gleek the monkey from Superfriends, says, in a slightly creepy yet playful tone of voice, I like to watch! as Johnny and Lisa roll around the bed in a pre-intercourse ritual revolving around rose petals, you know you are in for a very special movie. After a lengthy lovemaking scene (not to worry if you miss it the first time, they show it again in its entirety later in the movie) in which Tommy\\'s bizarre scaly torso and over-anatomized rear-end are lovingly depicted over and over again as he appears to hump Lisa\\'s hip, we discover that Lisa, for no particular reason, has become bored with Tommy's incessant lovemaking and decides to leave him. Just when you think the movie might lapse into an ordinary, pedestrian sort of badness, Johnny's best friend Mark, a man who\\'s job seems to be to wear James Brolin\\'s beard from Amityville Horror, shows up and electrifies the screen with a performance so wooden that it belongs in the lumber section of Home Depot. Incidentally, Mark is played by Greg Sestero, who, in addition to being described as a department store mannequin, was also the line producer on The Room and one of Tommy Wiseau\\'s five (5!!!!!) assistants on the movie. Lisa forces Mark, amid his paltry, unconvincing protests, to have an affair with her on their uncomfortable circular stairs. For no apparent reason Lisa decides that she is made of pure evil and wants to torture her angelic and insanely devoted fiancé, Johnny. Lisa receives pointed advice from her mother who casually announces that she is dying of breast cancer and then never mentions it again. But Lisa is determined to make Johnny\\'s life a living hell, in spite of the fact that she, according to her mother, cannot survive on her own in the cutthroat computer business. But not before they recycle the sex scene from earlier in the movie where we get another bird\\'s eye view of Johnny\\'s ludicrous naked body. Denny gets into trouble with a drug dealer. Mark shaves his beard. Tommy gets drunk on an unusual cocktail made from mixing whiskey and vodka. Lisa lies and tells everyone that Tommy hit her in a drunken rage. A balding psychologist appears out of nowhere, offers some advice, then apparently dies while softly falling on the ground in an attempt to catch a football thrown by Mark. All of these seemingly disparate events build up to two cathartic moments. The first is when Tommy expressively yells at Lisa with the line 'You are tearing me apart Lisa!'. You will cheer at this line as you realize that the film has been tearing you apart the whole time. And the second is at Tommy's birthday party where the worst actor that has ever been born plays a unidentified man wearing a silk shirt who utters a phrase that perfectly describes the experience of watching The Room, 'It feels like I'm sitting on atom bomb that is going to explode!' The shocking ending will leave you pleading for some kind of sequel. See this film at all costs. See it twice. Or three times. Or as one kid that I met from Woodland Hills has, 12 times! See it until you can recite every precious line of dialogue this movie has to offer. Let The Room become your new religion and Tommy Wiseau your prophet preaching the gospel according to Johnny. My dream is to someday buy a theater and run The Room 24 hours a day, 7 days a week until the print disintegrates. I hope it becomes your dream as well.\"\n",
    "neg, neu, pos = nb.test_single_document(negative_log_probs, neutral_log_probs, positive_log_probs, test_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 14, 11)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(neg), len(pos), len(neu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' With each viewing, The Room becomes more complexly entangled in and inseparable from my own life',\n",
       " ' If you took the greatest filmmakers in history and gave them all the task of purposefully creating a film as spectacularly horrible as this not one of them, with all their knowledge and skill, could make anything that could even be considered as a contender',\n",
       " ' Not one line or scene would rival any moment in The Room',\n",
       " ' The centerpiece of this filmic holocaust is Mr',\n",
       " ' All of these things are true at any one moment',\n",
       " ' You will fall in love with this man even as you are repelled by him from the first moment he steps onto screen with his long Louis the Fourteenth style black locks and thick triangular shoulders packed into an oddly fitting suit, and his metallic steroid destroyed skin',\n",
       " ' Tommy looks out of place, out of time and out of this world',\n",
       " ' Nor will there ever be',\n",
       " ' When Denny, who looks like the human version of Gleek the monkey from Superfriends, says, in a slightly creepy yet playful tone of voice, I like to watch! as Johnny and Lisa roll around the bed in a pre-intercourse ritual revolving around rose petals, you know you are in for a very special movie',\n",
       " \" Just when you think the movie might lapse into an ordinary, pedestrian sort of badness, Johnny's best friend Mark, a man who's job seems to be to wear James Brolin's beard from Amityville Horror, shows up and electrifies the screen with a performance so wooden that it belongs in the lumber section of Home Depot\",\n",
       " \" Incidentally, Mark is played by Greg Sestero, who, in addition to being described as a department store mannequin, was also the line producer on The Room and one of Tommy Wiseau's five (5!!!!!) assistants on the movie\",\n",
       " ' Lisa forces Mark, amid his paltry, unconvincing protests, to have an affair with her on their uncomfortable circular stairs',\n",
       " \" But Lisa is determined to make Johnny's life a living hell, in spite of the fact that she, according to her mother, cannot survive on her own in the cutthroat computer business\",\n",
       " \" But not before they recycle the sex scene from earlier in the movie where we get another bird's eye view of Johnny's ludicrous naked body\",\n",
       " ' Denny gets into trouble with a drug dealer',\n",
       " ' Mark shaves his beard',\n",
       " ' Tommy gets drunk on an unusual cocktail made from mixing whiskey and vodka',\n",
       " ' All of these seemingly disparate events build up to two cathartic moments',\n",
       " \" The first is when Tommy expressively yells at Lisa with the line 'You are tearing me apart Lisa!'\",\n",
       " ' See this film at all costs',\n",
       " ' My dream is to someday buy a theater and run The Room 24 hours a day, 7 days a week until the print disintegrates',\n",
       " ' I hope it becomes your dream as well']"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVM:\n",
    "    \n",
    "    def __init__(self, vocabulary, tokenisation):\n",
    "        self.dimensions = sorted(list(vocabulary))\n",
    "        self.tokenisation = tokenisation\n",
    "        \n",
    "    \n",
    "    def get_feature_vector(self, review_tokens):\n",
    "        review_tokens = set(review_tokens)\n",
    "        feature_vector = []\n",
    "        for word in self.dimensions:\n",
    "            if word in review_tokens:\n",
    "                feature_vector.append(1)\n",
    "            else:\n",
    "                feature_vector.append(0)\n",
    "        xs = np.array(feature_vector)\n",
    "\n",
    "        # Normalisation\n",
    "        denom = np.linalg.norm(xs)\n",
    "        \n",
    "        if denom == 0:\n",
    "            return xs\n",
    "\n",
    "        return xs / denom\n",
    "    \n",
    "    def get_test_data(self):\n",
    "        reviews = []\n",
    "        labels = []\n",
    "        to_search = './Datasets/neg/'\n",
    "        for f in os.listdir(to_search):\n",
    "            path = to_search+f\n",
    "            with open(path, 'r', encoding='latin-1') as fin:\n",
    "                reviews.append(fin.read().strip())\n",
    "                \n",
    "        labels.extend([-1] * 700)\n",
    "\n",
    "        to_search = './Datasets/pos/'\n",
    "        for f in os.listdir(to_search):\n",
    "            path = to_search+f\n",
    "            with open(path, 'r', encoding='latin-1') as fin:\n",
    "                reviews.append(fin.read().strip())\n",
    "                \n",
    "        labels.extend([1] * 700)\n",
    "                \n",
    "        reviews_sentences = list(map(lambda x: x.split('.'), reviews))\n",
    "        return reviews_sentences, labels\n",
    "    \n",
    "    \n",
    "    def train_classifier(self, negative_reviews, neutral_reviews, positive_reviews):\n",
    "        classifier = LinearSVC()\n",
    "        train_xs = []\n",
    "        train_ys = []\n",
    "\n",
    "        for neg_review in negative_reviews:\n",
    "            neg_review_tokens = self.tokenisation(neg_review)\n",
    "            xs = self.get_feature_vector(neg_review_tokens)\n",
    "            train_xs.append(xs)\n",
    "            train_ys.append(-1)  # Label -1 is for negative sentiment\n",
    "            \n",
    "        print(\"Neg train\")\n",
    "\n",
    "        for pos_review in positive_reviews:\n",
    "            pos_review_tokens = self.tokenisation(pos_review)\n",
    "            xs = self.get_feature_vector(pos_review_tokens)\n",
    "            train_xs.append(xs)\n",
    "            train_ys.append(1)  # Label 1 for positive sentiment\n",
    "            \n",
    "        print(\"Pos train\")\n",
    "            \n",
    "        for neu_review in neutral_reviews:\n",
    "            neu_review_tokens = self.tokenisation(neu_review)\n",
    "            xs = self.get_feature_vector(neu_review_tokens)\n",
    "            train_xs.append(xs)\n",
    "            train_ys.append(0)  # Label 0 for neutral sentiment\n",
    "            \n",
    "        print(\"Neu train\")\n",
    "        \n",
    "        classifier.fit(np.array(train_xs), np.array(train_ys))\n",
    "        return classifier\n",
    "    \n",
    "    \n",
    "    def test_classifier(self, classifier, add_full_stop=False):\n",
    "        test_xs = []\n",
    "        test_ys = []\n",
    "        reviews_test, labels_test = self.get_test_data()\n",
    "        \n",
    "        correct = 0\n",
    "        counter = 0\n",
    "        \n",
    "        for review, label in zip(reviews_test, labels_test):\n",
    "            counter += 1\n",
    "            if counter % 100 == 0:\n",
    "                print(counter)\n",
    "            \n",
    "            review_vector = []\n",
    "            for sentence in review:\n",
    "                if add_full_stop:\n",
    "                    sentence += '.'\n",
    "                sentence_tokens = self.tokenisation(sentence)\n",
    "                sentence_vector = self.get_feature_vector(sentence_tokens)\n",
    "                review_vector.append(sentence_vector)\n",
    "                \n",
    "            sentence_labels = classifier.predict(np.array(review_vector))\n",
    "            \n",
    "            neg_count = np.sum(sentence_labels == -1)\n",
    "            pos_count = np.sum(sentence_labels == 1)\n",
    "            \n",
    "            review_label = 3  # Some non-sensical value\n",
    "            if neg_count > pos_count:\n",
    "                review_label = -1\n",
    "            elif pos_count > neg_count:\n",
    "                review_label = 1\n",
    "            \n",
    "            if review_label == label:\n",
    "                correct += 1\n",
    "            \n",
    "#             sentence_labels += 1  # We are adding 1 so we can use np.bincount()\n",
    "#             review_label = np.argmax(np.bincount(sentence_labels)) - 1  # -1 since we added 1 to labels for np.bincount\n",
    "            \n",
    "#             if review_label == label:\n",
    "#                 correct += 1\n",
    "                \n",
    "        return correct / len(reviews_test)\n",
    "    \n",
    "    \n",
    "    def classify_single_document(self, classifier, review, add_full_stop=False):\n",
    "        review_sentences = test_review.split('.')\n",
    "        neg_sentences = []\n",
    "        neu_sentences = []\n",
    "        pos_sentences = []\n",
    "        \n",
    "        review_vector = []\n",
    "        for sentence in review_sentences:\n",
    "            if add_full_stop:\n",
    "                sentence += '.'\n",
    "            sentence_tokens = self.tokenisation(sentence)\n",
    "            sentence_vector = self.get_feature_vector(sentence_tokens)\n",
    "            review_vector.append(sentence_vector)\n",
    "        \n",
    "        review_sentences = np.array(review_sentences)\n",
    "        sentence_labels = classifier.predict(np.array(review_vector))\n",
    "        \n",
    "        neg_sentences = review_sentences[sentence_labels == -1]\n",
    "        neu_sentences = review_sentences[sentence_labels == 0]\n",
    "        pos_sentences = review_sentences[sentence_labels == 1]\n",
    "        \n",
    "        return list(neg_sentences), list(neu_sentences), list(pos_sentences)\n",
    "    \n",
    "    \n",
    "    def get_statistics(self):\n",
    "        classifier = self.train_classifier(negative_reviews_train, neutral_reviews_train, positive_reviews_train)\n",
    "        return self.test_classifier(classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neg train\n",
      "Pos train\n",
      "Neu train\n"
     ]
    }
   ],
   "source": [
    "cf = SVM(vocabulary, tokenisation)\n",
    "classifier = cf.train_classifier(negative_reviews_train, neutral_reviews_train, positive_reviews_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7085714285714285"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cf = SVM(vocabulary, tokenisation)\n",
    "cf.test_classifier(classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_review = \"I have now seen Mr. Tommy Wiseau's cinematic tour-de-force, 'The Room' three times. With each viewing, The Room becomes more complexly entangled in and inseparable from my own life. I no longer know where The Room ends and I begin. It is, without question, the worst film ever made. But this comment is in no way meant to be discouraging. Because while The Room is the worst movie ever made it is also the greatest way to spend a blisteringly fast 100 minutes in the dark. Simply put, The Room will change your life. It\\'s not just the dreadful acting or the sub-normal screenplay or the bewildering direction or the musical score so soaked in melodrama that you will throw up on yourself or the lunatic-making cinematography; no, there is something so magically wrong with this movie that it can only be the product of divine intervention. If you took the greatest filmmakers in history and gave them all the task of purposefully creating a film as spectacularly horrible as this not one of them, with all their knowledge and skill, could make anything that could even be considered as a contender. Not one line or scene would rival any moment in The Room. The centerpiece of this filmic holocaust is Mr. Tommy Wiseau himself. Without him, it would still be the worst movie ever made, but with him it is the greatest worst movie ever made. Tommy has been described as a Cajun, a Croatian cyborg, possibly from Belgium, clearly a product of Denmark, or maybe even not from this world or dimension. All of these things are true at any one moment. He is a tantalizing mystery stuffed inside an enigma wrapped in bacon and smothered in cheese. You will fall in love with this man even as you are repelled by him from the first moment he steps onto screen with his long Louis the Fourteenth style black locks and thick triangular shoulders packed into an oddly fitting suit, and his metallic steroid destroyed skin. Tommy looks out of place, out of time and out of this world. There has never been anything else like him. Nor will there ever be. The Room begins with Johnny (Tommy Wiseau) and his incomprehensibly evil fiancée Lisa (played by a woman with incongruously colored eyebrows and a propensity for removing her shirt) engaging in some light frottage, joined by, Denny, (played with a deft sense of the absurd by Phillip Haldiman), their sexually confused teenage neighbor who is clearly suffering from a form of aged decrepitude. When Denny, who looks like the human version of Gleek the monkey from Superfriends, says, in a slightly creepy yet playful tone of voice, I like to watch! as Johnny and Lisa roll around the bed in a pre-intercourse ritual revolving around rose petals, you know you are in for a very special movie. After a lengthy lovemaking scene (not to worry if you miss it the first time, they show it again in its entirety later in the movie) in which Tommy\\'s bizarre scaly torso and over-anatomized rear-end are lovingly depicted over and over again as he appears to hump Lisa\\'s hip, we discover that Lisa, for no particular reason, has become bored with Tommy's incessant lovemaking and decides to leave him. Just when you think the movie might lapse into an ordinary, pedestrian sort of badness, Johnny's best friend Mark, a man who\\'s job seems to be to wear James Brolin\\'s beard from Amityville Horror, shows up and electrifies the screen with a performance so wooden that it belongs in the lumber section of Home Depot. Incidentally, Mark is played by Greg Sestero, who, in addition to being described as a department store mannequin, was also the line producer on The Room and one of Tommy Wiseau\\'s five (5!!!!!) assistants on the movie. Lisa forces Mark, amid his paltry, unconvincing protests, to have an affair with her on their uncomfortable circular stairs. For no apparent reason Lisa decides that she is made of pure evil and wants to torture her angelic and insanely devoted fiancé, Johnny. Lisa receives pointed advice from her mother who casually announces that she is dying of breast cancer and then never mentions it again. But Lisa is determined to make Johnny\\'s life a living hell, in spite of the fact that she, according to her mother, cannot survive on her own in the cutthroat computer business. But not before they recycle the sex scene from earlier in the movie where we get another bird\\'s eye view of Johnny\\'s ludicrous naked body. Denny gets into trouble with a drug dealer. Mark shaves his beard. Tommy gets drunk on an unusual cocktail made from mixing whiskey and vodka. Lisa lies and tells everyone that Tommy hit her in a drunken rage. A balding psychologist appears out of nowhere, offers some advice, then apparently dies while softly falling on the ground in an attempt to catch a football thrown by Mark. All of these seemingly disparate events build up to two cathartic moments. The first is when Tommy expressively yells at Lisa with the line 'You are tearing me apart Lisa!'. You will cheer at this line as you realize that the film has been tearing you apart the whole time. And the second is at Tommy's birthday party where the worst actor that has ever been born plays a unidentified man wearing a silk shirt who utters a phrase that perfectly describes the experience of watching The Room, 'It feels like I'm sitting on atom bomb that is going to explode!' The shocking ending will leave you pleading for some kind of sequel. See this film at all costs. See it twice. Or three times. Or as one kid that I met from Woodland Hills has, 12 times! See it until you can recite every precious line of dialogue this movie has to offer. Let The Room become your new religion and Tommy Wiseau your prophet preaching the gospel according to Johnny. My dream is to someday buy a theater and run The Room 24 hours a day, 7 days a week until the print disintegrates. I hope it becomes your dream as well.\"\n",
    "cf = SVM(vocabulary, tokenisation)\n",
    "neg, neu, pos = cf.classify_single_document(classifier, test_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 10, 18)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(neg), len(pos), len(neu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' Simply put, The Room will change your life',\n",
       " ' Without him, it would still be the worst movie ever made, but with him it is the greatest worst movie ever made',\n",
       " ' He is a tantalizing mystery stuffed inside an enigma wrapped in bacon and smothered in cheese',\n",
       " ' There has never been anything else like him',\n",
       " ' The Room begins with Johnny (Tommy Wiseau) and his incomprehensibly evil fiancée Lisa (played by a woman with incongruously colored eyebrows and a propensity for removing her shirt) engaging in some light frottage, joined by, Denny, (played with a deft sense of the absurd by Phillip Haldiman), their sexually confused teenage neighbor who is clearly suffering from a form of aged decrepitude',\n",
       " \" Incidentally, Mark is played by Greg Sestero, who, in addition to being described as a department store mannequin, was also the line producer on The Room and one of Tommy Wiseau's five (5!!!!!) assistants on the movie\",\n",
       " ' Tommy gets drunk on an unusual cocktail made from mixing whiskey and vodka',\n",
       " ' Lisa lies and tells everyone that Tommy hit her in a drunken rage',\n",
       " ' See this film at all costs',\n",
       " ' My dream is to someday buy a theater and run The Room 24 hours a day, 7 days a week until the print disintegrates']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
