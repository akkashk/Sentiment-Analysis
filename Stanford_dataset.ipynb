{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from nltk import pos_tag, word_tokenize\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from IPython.core.debugger import set_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_reviews_train = []\n",
    "to_search = './Datasets/aclImdb/train/neg/'\n",
    "for f in os.listdir(to_search):\n",
    "    path = to_search + f\n",
    "    with open(path, 'r', encoding='latin-1') as fin:\n",
    "            negative_reviews_train.append(fin.read().strip())\n",
    "            \n",
    "positive_reviews_train = []\n",
    "to_search = './Datasets/aclImdb/train/pos/'\n",
    "for f in os.listdir(to_search):\n",
    "    path = to_search + f\n",
    "    with open(path, 'r', encoding='latin-1') as fin:\n",
    "            positive_reviews_train.append(fin.read().strip())\n",
    "            \n",
    "negative_reviews_test = []\n",
    "to_search = './Datasets/aclImdb/test/neg/'\n",
    "for f in os.listdir(to_search):\n",
    "    path = to_search + f\n",
    "    with open(path, 'r', encoding='latin-1') as fin:\n",
    "            negative_reviews_test.append(fin.read().strip())\n",
    "            \n",
    "positive_reviews_test= []\n",
    "to_search = './Datasets/aclImdb/test/pos/'\n",
    "for f in os.listdir(to_search):\n",
    "    path = to_search + f\n",
    "    with open(path, 'r', encoding='latin-1') as fin:\n",
    "            positive_reviews_test.append(fin.read().strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12500"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(negative_reviews_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenise_document(review, add_not_tag=False, unigram=False, bigram=False, pos=False, position=False):\n",
    "    \"\"\"\n",
    "    Given a review (string) we return a list of tokens\n",
    "    \"\"\"\n",
    "    \n",
    "    review_split = word_tokenize(review)\n",
    "    tokens = []\n",
    "    \n",
    "    if add_not_tag:\n",
    "        negation_word = {'not': True, \"isn't\": True, \"doesn't\": True, \"wasn't\":True, \"couldn't\": True, \"wouldn't\": True, \n",
    "                         \"didn't\": True}\n",
    "        punctuation = {'?': True, '!': True, '.': True, ',': True, ':': True, ';':True}\n",
    "\n",
    "        convert_word = False\n",
    "        for word in review_split:\n",
    "            if word in punctuation:\n",
    "                convert_word = False\n",
    "                tokens.append(word)\n",
    "                continue\n",
    "\n",
    "            if convert_word:\n",
    "                tokens.append('NOT_'+word)\n",
    "                continue\n",
    "\n",
    "            if word in negation_word:\n",
    "                convert_word = True\n",
    "                tokens.append(word)\n",
    "                continue\n",
    "            tokens.append(word)\n",
    "                \n",
    "                \n",
    "    if unigram:\n",
    "        tokens.extend(review_split)\n",
    "        \n",
    "        \n",
    "    if pos:\n",
    "        tags = pos_tag(review_split) # tags is a list of tuples: [(token, pos tag)]\n",
    "        # Append the POS tag to each word in 'tokens' to form a string. E.g. 'Peter' and 'NN' becomes 'Peter_NN'\n",
    "        # tokens have NOT tag added to them\n",
    "        tokens = [token+'_'+tag[1] for token, tag in zip(tokens, tags)]\n",
    "\n",
    "        \n",
    "    if bigram:\n",
    "        # Since one of the above two conditions will be fulfilled, 'tokens' will always have entries\n",
    "        for index in range(len(tokens) - 1):\n",
    "            word_1 = tokens[index]\n",
    "            word_2 = tokens[index + 1]\n",
    "            tokens[index] = word_1 + ' ' + word_2\n",
    "        tokens.pop()  # Take the last unigram word at end of tokens list\n",
    "\n",
    "        \n",
    "    if position:\n",
    "        # 'tokens' will already have been filled by this point by either 'unigram' or 'add_not_tag'\n",
    "        first_quarter_end = np.ceil(len(tokens) * 0.25)\n",
    "        middle_end = np.ceil(len(tokens) * 0.75)\n",
    "        # last_quarter = len(tokens)\n",
    "        \n",
    "        for index in range(len(tokens)):\n",
    "            if index < first_quarter_end:\n",
    "                to_append = 1\n",
    "            elif index < middle_end:\n",
    "                to_append = 2\n",
    "            else:\n",
    "                to_append = 3\n",
    "            tokens[index] = tokens[index]+'_'+str(to_append)\n",
    "        \n",
    "        \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocabulary(add_not_tag=False, unigram=False, bigram=False, pos=False, adjectives=False, position=False, length=16162):\n",
    "    neg_flatlist = negative_reviews_train + negative_reviews_test\n",
    "    pos_flatlist = positive_reviews_train + positive_reviews_test\n",
    "    data = neg_flatlist + pos_flatlist  # We have a list of textual reviews (strings)\n",
    "\n",
    "    freq = {}  # Since we are using dict, vocabulary will be unique\n",
    "    for review in data:\n",
    "        review_tokens = tokenise_document(review, unigram=unigram, add_not_tag=add_not_tag, bigram=bigram)\n",
    "\n",
    "        for token in review_tokens:\n",
    "            if token in freq:\n",
    "                freq[token] += 1\n",
    "            else:\n",
    "                freq[token] = 1\n",
    "                \n",
    "    if length is None:\n",
    "        cutoff = 7 if bigram else 4\n",
    "        vocabulary = set([token for token, count in freq.items() if count >= cutoff])\n",
    "    else:\n",
    "        sorted_freq = sorted([(count, token) for token, count in freq.items()], reverse=True)\n",
    "        vocabulary = set([token for _, token in sorted_freq[:length]])  # We use set to exploit O(1) lookup time\n",
    "    \n",
    "    \n",
    "    if pos or position:\n",
    "#         print(\"Entering POS/Position\")\n",
    "        vocabulary_pos = []\n",
    "        for i, review in enumerate(data):\n",
    "            \n",
    "#             if i % 100 == 0:\n",
    "#                 print(i)\n",
    "            \n",
    "            unigram_tokens = np.array(tokenise_document(review, unigram=unigram, add_not_tag=add_not_tag))\n",
    "            pos_tokens = np.array(tokenise_document(review, unigram=unigram, add_not_tag=add_not_tag, pos=pos, position=position))\n",
    "            mask = list(map(lambda token: token in vocabulary, unigram_tokens))\n",
    "            vocabulary_pos.extend(list(pos_tokens[mask]))\n",
    "        \n",
    "        freq = {}\n",
    "        for token in vocabulary_pos:\n",
    "            if token in freq:\n",
    "                freq[token] += 1\n",
    "            else:\n",
    "                freq[token] = 1\n",
    "\n",
    "        if length is None:\n",
    "            vocabulary = set([token for token, count in freq.items() if count >= 4])\n",
    "        else:\n",
    "            sorted_freq = sorted([(count, token) for token, count in freq.items()], reverse=True)\n",
    "            vocabulary = set([token for _, token in sorted_freq[:length]])\n",
    "            \n",
    "        \n",
    "    if adjectives:\n",
    "        # Filter the 'sored_freq' list to only contains words with tags that end in JJ/JJR/JJS\n",
    "        freq = {}\n",
    "        for review in data:\n",
    "            review_tokens = tokenise_document(review, unigram=unigram, add_not_tag=add_not_tag, pos=True)\n",
    "\n",
    "            for pos_token in review_tokens:\n",
    "                \n",
    "                if pos_token.endswith('JJ'):\n",
    "                    token = pos_token[:-3]\n",
    "                elif pos_token.endswith('JJR'):\n",
    "                    token = pos_token[:-4]\n",
    "                elif pos_token.endswith('JJS'):\n",
    "                    token = pos_token[:-4]\n",
    "                \n",
    "                if token in freq:\n",
    "                    freq[token] += 1\n",
    "                else:\n",
    "                    freq[token] = 1\n",
    "\n",
    "        sorted_freq = sorted([(count, token) for token, count in freq.items()], reverse=True)\n",
    "        vocabulary = set([token for _, token in sorted_freq[:length]])\n",
    "    \n",
    "    return vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayes:\n",
    "    \n",
    "    def __init__(self, vocabulary, tokenisation):\n",
    "        self.vocabulary = vocabulary\n",
    "        self.tokenisation = tokenisation\n",
    "\n",
    "        \n",
    "    def get_document_vector(self, document_tokens):\n",
    "        freq = {v:0 for v in self.vocabulary}\n",
    "        for word in document_tokens:\n",
    "            if word in self.vocabulary:\n",
    "                freq[word] = 1\n",
    "        return freq\n",
    "    \n",
    "    \n",
    "    def get_class_probabilities(self, class_data):\n",
    "        \"\"\"\n",
    "        class_data: A list containing all the tokens in a training data document. P(f_i|c) from training documents\n",
    "        \"\"\"\n",
    "        # We start with the add-one smoothing\n",
    "        log_prob_dict = {v:1 for v in self.vocabulary}  # We encode the add one smoothing count at start\n",
    "        \n",
    "        total_class_tokens = 0  # Tokens contained in vocabulary\n",
    "        for word in class_data:\n",
    "            if word in self.vocabulary:\n",
    "                log_prob_dict[word] += 1\n",
    "                total_class_tokens += 1\n",
    "                \n",
    "        # We divide by the denominator and log the values\n",
    "        for key in log_prob_dict.keys():\n",
    "            log_prob_dict[key] = np.log(log_prob_dict[key] / (total_class_tokens + len(self.vocabulary)))\n",
    "\n",
    "        return log_prob_dict\n",
    "\n",
    "    \n",
    "    def train_probabilities(self):\n",
    "        neg_train_tokens = []\n",
    "        for neg_review in negative_reviews_train:\n",
    "            neg_review = self.tokenisation(neg_review)\n",
    "            neg_train_tokens.extend(neg_review)\n",
    "        negative_log_probs = self.get_class_probabilities(neg_train_tokens)\n",
    "        \n",
    "        pos_train_tokens = []\n",
    "        for pos_review in positive_reviews_train:\n",
    "            pos_review = self.tokenisation(pos_review)\n",
    "            pos_train_tokens.extend(pos_review)\n",
    "        positive_log_probs = self.get_class_probabilities(pos_train_tokens)\n",
    "    \n",
    "        return negative_log_probs, positive_log_probs\n",
    "    \n",
    "    \n",
    "    def test_documents(self, negative_log_probs, positive_log_probs):\n",
    "        correct = 0\n",
    "        for i, pos_review in enumerate(positive_reviews_test):\n",
    "            \n",
    "            if i%1000 == 0:\n",
    "                print(i)\n",
    "            \n",
    "            pos_review = self.tokenisation(pos_review)\n",
    "            document_vector = self.get_document_vector(pos_review)\n",
    "            \n",
    "            neg_sum = 0   # For negative class\n",
    "            pos_sum = 0   # For positive class\n",
    "            for word in document_vector.keys():\n",
    "                neg_sum += document_vector[word] * negative_log_probs[word]\n",
    "                pos_sum += document_vector[word] * positive_log_probs[word]\n",
    "\n",
    "            if pos_sum > neg_sum:\n",
    "                correct += 1\n",
    "                \n",
    "        print(\"Calculated test positives\")\n",
    "\n",
    "        for i, neg_review in enumerate(negative_reviews_test):\n",
    "            \n",
    "            if i%1000 == 0:\n",
    "                print(i)\n",
    "            \n",
    "            neg_review = self.tokenisation(neg_review)\n",
    "            document_vector = self.get_document_vector(neg_review)\n",
    "            \n",
    "            neg_sum = 0   # For negative class\n",
    "            pos_sum = 0   # For positive class\n",
    "            for word in document_vector.keys():\n",
    "                neg_sum += document_vector[word] * negative_log_probs[word]\n",
    "                pos_sum += document_vector[word] * positive_log_probs[word]\n",
    "                \n",
    "            if neg_sum > pos_sum:\n",
    "                correct += 1\n",
    "        \n",
    "        print(\"Calculated test negatives\")\n",
    "\n",
    "        return correct / (len(positive_reviews_test) + len(negative_reviews_test))\n",
    "    \n",
    "    \n",
    "    def get_statistics(self):\n",
    "        neg_log_prob, pos_log_prob = self.train_probabilities()\n",
    "        print(\"Calculated train probabilities\")\n",
    "        accuracy = self.test_documents(neg_log_prob, pos_log_prob)\n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = get_vocabulary(add_not_tag=True)\n",
    "print(len(vocabulary))\n",
    "tokenisation = lambda x: tokenise_document(x, add_not_tag=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculated train probabilities\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "Calculated test positives\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "Calculated test negatives\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.83112"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NaiveBayes(vocabulary, tokenisation).get_statistics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVM:\n",
    "    \n",
    "    def __init__(self, vocabulary, tokenisation):\n",
    "        self.dimensions = sorted(list(vocabulary))\n",
    "        self.tokenisation = tokenisation\n",
    "        \n",
    "    \n",
    "    def get_feature_vector(self, review_tokens):\n",
    "        review_tokens = set(review_tokens)\n",
    "        feature_vector = []\n",
    "        for word in self.dimensions:\n",
    "            if word in review_tokens:\n",
    "                feature_vector.append(1)\n",
    "            else:\n",
    "                feature_vector.append(0)\n",
    "        xs = np.array(feature_vector)\n",
    "\n",
    "        # Normalisation\n",
    "        denom = np.linalg.norm(xs)\n",
    "\n",
    "        return xs / denom\n",
    "    \n",
    "    \n",
    "    def get_train_test_data(self):\n",
    "        train_xs = []\n",
    "        train_ys = []\n",
    "\n",
    "        test_xs = []\n",
    "        test_ys = []\n",
    "\n",
    "        for neg_review in negative_reviews_train:\n",
    "            neg_review_tokens = self.tokenisation(neg_review)\n",
    "            xs = self.get_feature_vector(neg_review_tokens)\n",
    "            train_xs.append(xs)\n",
    "            train_ys.append(-1)  # Label -1 is for negative sentiment\n",
    "            \n",
    "        print(\"DONE 1\")\n",
    "\n",
    "        for pos_review in positive_reviews_train:\n",
    "            pos_review_tokens = self.tokenisation(pos_review)\n",
    "            xs = self.get_feature_vector(pos_review_tokens)\n",
    "            train_xs.append(xs)\n",
    "            train_ys.append(1)  # Label 1 for positive sentiment\n",
    "            \n",
    "        print(\"DONE 2\")\n",
    "\n",
    "        for pos_review in positive_reviews_test:\n",
    "            pos_review_tokens = self.tokenisation(pos_review)\n",
    "            xs = self.get_feature_vector(pos_review_tokens)\n",
    "            test_xs.append(xs)\n",
    "            test_ys.append(1)  # Label 1 for positive sentiment\n",
    "            \n",
    "        print(\"DONE 3\")\n",
    "\n",
    "        for neg_review in negative_reviews_test:\n",
    "            neg_review_tokens = self.tokenisation(neg_review)\n",
    "            xs = self.get_feature_vector(neg_review_tokens)\n",
    "            test_xs.append(xs)\n",
    "            test_ys.append(-1)  # Label -1 for negatuve sentiment\n",
    "            \n",
    "        print(\"DONE 4\")\n",
    "\n",
    "        return train_xs, train_ys, test_xs, test_ys\n",
    "    \n",
    "    \n",
    "    def get_statistics(self):\n",
    "        classifier = SVC(kernel='linear')\n",
    "        train_xs, train_ys, test_xs, test_ys = self.get_train_test_data()\n",
    "        print(\"Gotten train/test split\")\n",
    "        classifier.fit(train_xs, train_ys)\n",
    "        print(\"Fitted data\")\n",
    "        accuracy = classifier.score(test_xs, test_ys)\n",
    "\n",
    "        return accuracy\n",
    "    \n",
    "    \n",
    "    def use_svm_light(self, train_xs, train_ys, test_xs, test_ys):\n",
    "        with open('./train.txt', 'w') as fout:\n",
    "            for vector, label in zip(train_xs, train_ys):\n",
    "                vector_ls = [str(label)]\n",
    "                for index, value in enumerate(vector):\n",
    "                    # Model needs feature numbers to start from 1\n",
    "                    vector_ls.append(str(index+1)+':'+str(value))\n",
    "                # NEED NEWLINE CHARACTER AT END. PYTHON AUTOMATICALLY CONVERTS THIS TO APPROPRIATE ENDING\n",
    "                line = ' '.join(vector_ls)+'\\n'\n",
    "                fout.write(line)\n",
    "\n",
    "        with open('./test.txt', 'w') as fout:\n",
    "            for vector, label in zip(test_xs, test_ys):\n",
    "                vector_ls = [str(label)]\n",
    "                for index, value in enumerate(vector):\n",
    "                    vector_ls.append(str(index+1)+':'+str(value))\n",
    "                line = ' '.join(vector_ls)+'\\n'\n",
    "                fout.write(line)\n",
    "\n",
    "    \n",
    "    def get_statistics_svmlight(self):\n",
    "        train_xs, train_ys, test_xs, test_ys = self.get_train_test_data()\n",
    "        self.use_svm_light(train_xs, train_ys, test_xs, test_ys)\n",
    "        # Manually call from bash after writing training and testing document using \n",
    "        # ./svm_learn train.txt model.txt\n",
    "        # ./svm_classify test.txt model.txt output.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE 1\n",
      "DONE 2\n",
      "DONE 3\n",
      "DONE 4\n",
      "Gotten train/test split\n"
     ]
    }
   ],
   "source": [
    "# We use same vocabulary as NB above, so no need to recalculate\n",
    "SVM(vocabulary, tokenisation).get_statistics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
