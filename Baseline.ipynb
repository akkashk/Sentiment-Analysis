{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO\n",
    "\n",
    "- Get the general framework working, reading from original dataset, etc.\n",
    "- Use decision tree to get baseline model results with manual list\n",
    "- Getting unigram model working with feature vector and all\n",
    "- Use all three/two classifiers in Proposal to work with unigram model and get results with \n",
    "- Choose one of the other testing model and get it to work with the above classifiers. Repeat this until all tests done\n",
    "- Look at the README file in datatset and write the info from there in report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'neg' and 'pos' subfolders contain the following:\n",
    "negative_reviews = [[], [], []]\n",
    "to_search = './Datasets/neg_org/'\n",
    "for f in os.listdir(to_search):\n",
    "    path = to_search+f\n",
    "    fold = f[:5]\n",
    "    if fold < 'cv233':\n",
    "        fold_index = 0\n",
    "    elif fold < 'cv466':\n",
    "        fold_index = 1\n",
    "    else:\n",
    "        fold_index = 2\n",
    "    with open(path, 'r', encoding='latin-1') as fin:\n",
    "            negative_reviews[fold_index].append(fin.read().strip())  # We remove the ending newline characters\n",
    "            \n",
    "positive_reviews = [[], [], []]\n",
    "to_search = './Datasets/pos_org/'\n",
    "for f in os.listdir(to_search):\n",
    "    path = to_search+f\n",
    "    fold = f[:5]\n",
    "    if fold < 'cv233':\n",
    "        fold_index = 0\n",
    "    elif fold < 'cv466':\n",
    "        fold_index = 1\n",
    "    else:\n",
    "        fold_index = 2\n",
    "    with open(path, 'r', encoding='latin-1') as fin:\n",
    "            positive_reviews[fold_index].append(fin.read().strip())  # We remove the ending newline characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Although in Section 3 they give a figure for corpus, in Figure 1 they actually only use 700 pos/neg reviews to give 50% \n",
    "# baseline value for results\n",
    "\n",
    "human1_pos = ['dazzling', 'brilliant', 'phenomenal', 'excellent', 'fantastic']\n",
    "human1_neg = ['suck', 'terrible', 'awful', 'unwatchable', 'hideous']\n",
    "\n",
    "human2_pos = ['gripping', 'mesmerizing', 'riveting', 'spectacular', 'cool', 'awesome', 'thrilling', 'badass', 'excellent', \n",
    "              'moving', 'exciting']\n",
    "human2_neg = ['bad', 'cliched', 'sucks', 'boring', 'stupid', 'slow']\n",
    "\n",
    "human3_pos = ['love', 'wonderful', 'best', 'great', 'superb', 'still', 'beautiful']\n",
    "human3_neg = ['bad', 'worst', 'stupid', 'waste', 'boring', '?', '!']\n",
    "\n",
    "\n",
    "def convert_str_to_dict(review):\n",
    "    \"\"\"\n",
    "    Returns a dictionary of tokens in review along with the frequency of those tokens\n",
    "    \"\"\"\n",
    "    freq = {}\n",
    "    for word in review.split():\n",
    "        if word in freq:\n",
    "            freq[word] += 1\n",
    "        else:\n",
    "            freq[word] = 1\n",
    "    return freq\n",
    "\n",
    "\n",
    "def return_sentiment_tag_counts(pos_tokens, neg_tokens, review):\n",
    "    \"\"\"\n",
    "    Given a list of tokens we use to denote positive and negative sentiment, we return a dictionary of values such that \n",
    "    {sentiment_token: frequency of token in review} for both positive and negative sentiments\n",
    "    \"\"\"\n",
    "    pos = {w:0 for w in pos_tokens}\n",
    "    neg = {w:0 for w in neg_tokens}\n",
    "    review_tokens = convert_str_to_dict(review)\n",
    "    \n",
    "    for p in pos_tokens:\n",
    "        if p in review_tokens:\n",
    "            pos[p] += review_tokens[p]\n",
    "    for n in neg_tokens:\n",
    "        if n in review_tokens:\n",
    "            neg[n] += review_tokens[n]\n",
    "    \n",
    "    return pos, neg\n",
    "\n",
    "\n",
    "def get_statistics(pos_tokens, neg_tokens):\n",
    "    ties = 0\n",
    "    \n",
    "    neg_correct = 0\n",
    "    for neg_fold in negative_reviews:\n",
    "        for neg_review in neg_fold:\n",
    "            pos_dict, neg_dict = return_sentiment_tag_counts(pos_tokens, neg_tokens, neg_review)\n",
    "            pos_count = sum(pos_dict.values())\n",
    "            neg_count = sum(neg_dict.values())\n",
    "            if pos_count == neg_count:\n",
    "                ties += 1\n",
    "            if neg_count > pos_count:\n",
    "                neg_correct += 1\n",
    "\n",
    "    pos_correct = 0\n",
    "    for pos_fold in positive_reviews:\n",
    "        for pos_review in pos_fold:\n",
    "            pos_dict, neg_dict = return_sentiment_tag_counts(pos_tokens, neg_tokens, pos_review)\n",
    "            pos_count = sum(pos_dict.values())\n",
    "            neg_count = sum(neg_dict.values())\n",
    "            if pos_count == neg_count:\n",
    "                ties += 1\n",
    "            if pos_count > neg_count:\n",
    "                pos_correct += 1\n",
    "            \n",
    "    total_correct = pos_correct + neg_correct\n",
    "    accuracy = (total_correct / 1400) * 100\n",
    "    tied_precentage = (ties / 1400) * 100\n",
    "    \n",
    "    return accuracy, tied_precentage\n",
    "\n",
    "acc1, tied1 = get_statistics(human1_pos, human1_neg)\n",
    "acc2, tied2 = get_statistics(human2_pos, human2_neg)\n",
    "acc3, tied3 = get_statistics(human3_pos, human3_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Proposed word list</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Ties</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Human 1</td>\n",
       "      <td>19.5</td>\n",
       "      <td>75.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Human 2</td>\n",
       "      <td>41.3</td>\n",
       "      <td>39.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Human 3</td>\n",
       "      <td>60.4</td>\n",
       "      <td>15.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Proposed word list  Accuracy  Ties\n",
       "0            Human 1      19.5  75.0\n",
       "1            Human 2      41.3  39.4\n",
       "2            Human 3      60.4  15.5"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [('Human 1', round(acc1, 1), round(tied1, 1)), \n",
    "        ('Human 2', round(acc2, 1), round(tied2, 1)), \n",
    "        ('Human 3', round(acc3, 1), round(tied3, 1))]\n",
    "summary = pd.DataFrame(data, columns=['Proposed word list', 'Accuracy', 'Ties'])\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'brilliant': 0,\n",
       "  'dazzling': 0,\n",
       "  'excellent': 0,\n",
       "  'fantastic': 0,\n",
       "  'phenomenal': 0},\n",
       " {'awful': 0, 'hideous': 0, 'suck': 0, 'terrible': 0, 'unwatchable': 0})"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = negative_reviews[1][37]\n",
    "return_sentiment_tag_counts(human1_pos, human1_neg, r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why we can't reproduce results:\n",
    "\n",
    "They break ties with a policy that 'maximises the accuracy' (paragraph below Figure 2). So part of the count towards Ties is used for accuracy too but we're not told how it is done. So cannot reproduce accuracy results.\n",
    "\n",
    "Why do ties differ??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
